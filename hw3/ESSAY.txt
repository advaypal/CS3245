Replace this file with the answers to the essay questions here.
----------------------------------------------------------------------

1. In this assignment, we didn't ask you to support phrasal queries, which is a feature that is typically supported in web search engines. Describe how you would support phrasal search in conjunction with the VSM model. A sketch of the algorithm is sufficient. (For those of you who like a challenge, please go ahead and implement this feature in your submission but clearly demarcate it in your code and allow this feature to be turned on or off using the command line switch "-x" (where "-x" means to turn on the extended processing of phrasal queries). We will give a small bonus to submissions that achieve this functionality correctly).

Ans: Phrasal queries can be supported in the VSM by using k-grams as index terms rather than unigrams, and then using the same algorithm for single terms. This is similar to the biword indices idea of the Boolean Retrieval model. Unfortunately, this increases our space requirement quite a bit. Another way of supporting phrase queries would be to look at positional indices just as in the case for Boolean Retrieval model. For a given query, we see the difference between the positions of particular terms in the query, and rank documents based on whether the given terms appear in them at the same index difference, similar to the case of the boolean retrieval model. 

2. Describe how your search engine reacts to long documents and long queries as compared to short documents and queries. Is the normalization you use sufficient to address the problems (see Section 6.4.4 for a hint)? In your judgement, is the ltc.lnc scheme (n.b., not the ranking scheme you were asked to implement) sufficient for retrieving documents from the Reuters-21578 collection?

Ans: In Longer Documents and queries, each term loses relevance because the denominator is larger due to the increased length of the document. Therefore, the search engine is likely to rank shorter documents higher than longer ones.

According to 6.4.4, the normalization we use can raise the score of longer documents due to the increased document length. Therefore, our normalisation scheme, while addressing the problem, creates a new problem of longer documents being ranked higher. We can solve this using pivoted document length normalization, as described in the text.

The ltc lnc scheme is different from the scheme we implemented in that it asks us to consider the idf values for terms in the query rather than in the document. However, this does not make a difference, as we only need to consider the idf value in one case. Hence, I feel that this is not sufficient for retrieving documents.


3. Do you think zone or field parametric indices would be useful for practical search in the Reuters collection? Note: the Reuters collection does have metadata for each article but the quality of the metadata is not uniform, nor are the metadata classifications uniformly applied (some documents have it, some don't). Hint: for the next Homework #4, we will be using field metadata, so if you want to base Homework #4 on your Homework #3, you're welcomed to start support of this early (although no extra credit will be given if it's right)

Ans: Yes I think zone or field parametric indices would be useful for practical search, especially if we are looking for particular metadata, like a given date or a given author. Even though the classifications are not uniformly applied(would lead to loss in recall), and the classifications are not uniform, we can still narrow our search for particular types of meta data by making use of zone/field parametric indices.
